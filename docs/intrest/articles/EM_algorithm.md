---
title: "EM算法详解"
date: 2020-04-17T23:54:13+08:00
lastmod: 2020-04-17T23:54:13+08:00
description: ""
tags: ["概率论", "数学", "机器学习", "算法"]
categories: ["概率论", "算法"]
author: "转载"
comment: true
toc: true
autoCollapseToc: true
postMetaInFooter: false
hiddenFromHomePage: false
contentCopyright: true
reward: true
mathjax: true
mathjaxEnableSingleDollar: false
mathjaxEnableAutoNumber: false
---


>转载的目的在于学习和存档，因为其他云存档文件存在丢失可能，如有侵权联系删除。

>原文地址：https://www.julyedu.com/question/big/kp_id/23/ques_id/1007



## 一 什么是极大似然

到底什么是EM算法呢？Wikipedia给的解释是：

>最大期望算法（Expectation-maximization algorithm，又译为期望最大化算法），是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量。

最大期望算法经过两个步骤交替进行计算，

第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；

第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。






### 1.1 似然函数


你懂了么？十有八九你没懂。因为你可能不懂什么是最大似然估计？而想了解最大似然估计，又得先从似然函数开始。但什么又是似然函数

在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性。

而极大似然就相当于最大可能的意思。

比如你一位同学和一位猎人一起外出打猎，一只野兔从前方窜过。只听一声枪响，野兔应声到下，如果要你推测，这一发命中的子弹是谁打的？你就会想，只发一枪便打中，由于猎人命中的概率一般大于你那位同学命中的概率，从而推断出这一枪应该是猎人射中的。

这个例子所作的推断就体现了最大似然法的基本思想。

多数情况下我们是根据已知条件来推算结果，而最大似然估计是已经知道了结果，然后寻求使该结果出现的可能性最大的条件，以此作为估计值。

看到没，概率是根据条件推测结果，而似然则是根据结果反推条件。在这种意义上，似然函数可以理解为条件概率的逆反。

假定已知某个参数B时，推测事件A会发生的概率写作：

![公式一](/images/EM_algorithm/em1.svg)


 通过贝叶斯公式，可以得出:

![公式2](/images/EM_algorithm/em2.svg)

现在我们反过来：事件A发生已经了，请通过似然函数，估计参数B的可能性。

一上公式，你可能就懵圈了。然后回想起我前沿开头所说的话：难道就没有一篇通俗易懂的么？

答案，当然是有的。我们从一个具体的例子人手。


### 1.2 似然函数举例：已知样本X，求参数θ

自从Google的围棋机器人AlphaGo通过4:1战胜人类世界冠军李世石之后，人工智能的大潮便一发不可收拾，在无人驾驶、人脸识别、安防监控、医疗影像等各领域大行其道。而专注AI教育的七月在线也已于2017年年底累积了10万AI学员。

假定我们需要统计七月在线10万学员中男生女生的身高分布，怎么统计呢？考虑到10万的数量巨大，所以不可能一个一个的去统计。对的，随机抽样，从10万学员中随机抽取100个男生，100个女生，然后依次统计他们各自的身高。

对于这个问题，我们通过数学建模抽象整理下

1) 首先我们从10万学员中抽取到100个男生/女生的身高，组成样本集X，X={x1,x2,…,xN}，其中xi表示抽到的第i个人的身高，N等于100，表示抽到的样本个数。

2) 假定男生的身高服从正态分布![](/images/EM_algorithm/em3.svg)，女生的身高则服从另一个正态分布：![](/images/EM_algorithm/em4.svg)。

3) 但是这两个分布的均值u和方差∂2都不知道（别问我，均值是啥，方差又是啥，请查Google或Wikipedia），设为未知参数θ=[u, ∂]T

4) 现在需要用极大似然法（MLE），通过这100个男生或100个女生的身高结果，即样本集X来估计两个正态分布的未知参数θ，问题定义相当于已知X，求θ，换言之就是求p(θ|x)

因为这些男生（的身高）是服从同一个高斯分布p(x|θ)的。那么抽到男生A（的身高）的概率是p(xA|θ)，抽到男生B的概率是p(xB|θ)，考虑到他们是独立的，所以同时抽到男生A和男生B的概率是p(xA|θ)* p(xB|θ)。

同理，我从分布是p(x|θ)的总体样本中同时抽到这100个男生样本的概率，也就是样本集X中100个样本的联合概率（即它们各自概率的乘积），用下式表示：

![](/images/EM_algorithm/em5.jpg)

现在需要使θ的似然函数L(θ)极大化，然后极大值对应的θ就是我们的估计。

对于求一个函数的极值，通过我们在本科所学的微积分知识，最直接的设想是求导，然后让导数为0，那么解这个方程得到的θ就是了（当然，前提是函数L(θ)连续可微）。但，如果θ是包含多个参数的向量那怎么处理呢？当然是求L(θ)对所有参数的偏导数，也就是梯度了，从而n个未知的参数，就有n个方程，方程组的解就是似然函数的极值点了，最终得到这n个参数的值。

求极大似然函数估计值的一般步骤：

①写出似然函数；

②对似然函数取对数，并整理；

③求导数，令导数为0，得到似然方程；

④解似然方程，得到的参数即为所求；


## 二、EM算法的通俗理解

2.1 极大似然估计的复杂情形

我们已经知道，极大似然估计用一句话概括就是：知道结果，反推条件θ。

例如，在上文中，为了统计七月在线10万学员中男生女生的身高分布

1. 首先我们从10万学员中抽取到100个男生/女生的身高，组成样本集X，X={x1,x2,…,xN}，其中xi表示抽到的第i个人的身高，N等于100，表示抽到的样本个数。
2. 假定男生的身高服从正态分布 ，女生的身高则服从另一个正态分布：  ，但是这两个分布的均值u和方差∂2都不知道，设为未知参数θ=[u, ∂]^T
3. 现在需要用极大似然法（MLE），通过这100个男生或100个女生的身高结果，即样本集X来估计两个正态分布的未知参数θ，问题定义相当于已知X，求θ，换言之就是求p(θ|x)

极大似然估计的目标是求解实现结果的最佳参数θ，但极大似然估计需要面临的概率分布只有一个或者知道结果是通过哪个概率分布实现的，只不过你不知道这个概率分布的参数。

但现在我们让情况复杂一点，比如这100个男生和100个女生混在一起了。我们拥有200个人的身高数据，却不知道这200个人每一个是男生还是女生，此时的男女性别就像一个隐变量。

这时候情况就有点尴尬，因为通常来说，我们只有知道了精确的男女身高的正态分布参数我们才能知道每一个人更有可能是男生还是女生。反过来，我们只有知道了每个人是男生还是女生才能尽可能准确地估计男女各自身高的正态分布的参数。

而EM算法就是为了解决“极大似然估计”这种更复杂而存在的。

### 2.2 EM算法中的隐变量

理解EM算法中的隐变量很关键，就如理解KMP那必须得理解NEXT数组的意义一样。

一般的用Y表示观测到的随机变量的数据，Z表示隐随机变量的数据(因为我们观测不到结果是从哪个概率分布中得出的，所以将这个叫做隐变量)。于是Y和Z连在一起被称为完全数据，仅Y一个被称为不完全数据。

这时有没有发现EM算法面临的问题主要就是：有个隐变量数据Z。而如果Z已知的话，那问题就可用极大似然估计求解了。 于是乎，怎么把Z变成已知的？

再举第二个日常生活的例子。

假定你是一五星级酒店的厨师，现在需要把锅里的菜平均分配到两个碟子里。如果只有一个碟子乘菜那就什么都不用说了，但问题是有2个碟子，正因为根本无法估计一个碟子里应该乘多少菜，所以无法一次性把菜完全平均分配。

解法：

1) 大厨先把锅里的菜一股脑倒进两个碟子里，然后看看哪个碟子里的菜多，就把这个碟子中的菜往另一个碟子中匀匀，之后重复多次匀匀的过程，直到两个碟子中菜的量大致一样。 上面的例子中，平均分配这个结果是“观测数据z”，为实现平均分配而给每个盘子分配多少菜是“待求参数θ”，分配菜的手感就是“概率分布”。
2) 于是若只有一个盘子，那概率分布就确定了（“把锅里的菜全部倒到一个盘子”这样的手感是个人都有吧），而因为有两个盘子，所以“给一个盘子到多少菜才好”的手感就有些模糊不定，不过我们可以采用上面的解法来实现最终目标。

EM算法的思想就是：

①给θ自主规定个初值（既然我不知道想实现“两个碟子平均分配锅里的菜”的话每个碟子需要有多少菜，那我就先估计个值）；

②根据给定观测数据和当前的参数θ，求未观测数据z的条件概率分布的期望（在上一步中，已经根据手感将菜倒进了两个碟子，然后这一步根据“两个碟子里都有菜”和“当前两个碟子都有多少菜”来判断自己倒菜的手感）；

③上一步中z已经求出来了，于是根据极大似然估计求最优的θ’（手感已经有了，那就根据手感判断下盘子里应该有多少菜，然后把菜匀匀）；

④因为第二步和第三步的结果可能不是最优的，所以重复第二步和第三步，直到收敛（重复多次匀匀的过程，直到两个碟子中菜的量大致一样）。

而上面的第二步被称作E步（求期望），第三步被称作M步（求极大化），从而不断的E、M。


### 2.3 EM算法的第三个例子：抛硬币

Nature Biotech在他的一篇EM tutorial文章《Do, C. B., & Batzoglou, S. (2008). What is the expectation maximization algorithm?. Nature biotechnology, 26(8), 897.》中，用了一个投硬币的例子来讲EM算法的思想。

比如两枚硬币A和B，如果知道每次抛的是A还是B，那可以直接估计（见下图a）。

如果不知道抛的是A还是B（这时就刺激了吧，对的，这就是咱们不知情的隐变量），只观测到5轮循环每轮循环10次，共计50次投币的结果，这时就没法直接估计A和B的正面概率。这时，就轮到EM算法出场了（见下图b）。

![](/images/EM_algorithm/em6.jpg)


可能咋一看，你没看懂。没关系，我们来通俗化这个例子。

还是两枚硬币A和B，假定随机抛掷后正面朝上概率分别为PA，PB。为了估计这两个硬币朝上的概率，咱们轮流抛硬币A和B，每一轮都连续抛5次，总共5轮：

硬币	结果	统计
```bash
A	正正反正反	3正-2反
B	反反正正反	2正-3反
A	正反反反反	1正-4反
B	正反反正正	3正-2反
A	反正正反反	2正-3反
```

硬币A被抛了15次，在第一轮、第三轮、第五轮分别出现了3次正、1次正、2次正，所以很容易估计出PA，类似的，PB也很容易计算出来，如下：

PA = （3+1+2）/ 15 = 0.4
PB= （2+3）/10 = 0.5

问题来了，如果我们不知道抛的硬币是A还是B呢（即硬币种类是隐变量），然后再轮流抛五轮，得到如下结果：

硬币	结果	统计
```bash
Unknown	正正反正反	3正-2反
Unknown	反反正正反	2正-3反
Unknown	正反反反反	1正-4反
Unknown	正反反正正	3正-2反
Unknown	反正正反反	2正-3反
```
OK，问题变得有意思了。现在我们的目标没变，还是估计PA和PB，需要怎么做呢？

显然，此时我们多了一个硬币种类的隐变量，设为z，可以把它认为是一个5维的向量（z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币，比如z1，就代表第一轮投掷时使用的硬币是A还是B。

1) 但是，这个变量z不知道，就无法去估计PA和PB，所以，我们必须先估计出z，然后才能进一步估计PA和PB。

2) 可要估计z，我们又得知道PA和PB，这样我们才能用极大似然概率法则去估计z，这不是鸡生蛋和蛋生鸡的问题吗，如何破？

答案就是先随机初始化一个PA和PB，用它来估计z，然后基于z，还是按照最大似然概率法则去估计新的PA和PB，如果新的PA和PB和我们初始化的PA和PB一样，请问这说明了什么？

这说明我们初始化的PA和PB是一个相当靠谱的估计！

就是说，我们初始化的PA和PB，按照最大似然概率就可以估计出z，然后基于z，按照最大似然概率可以反过来估计出P1和P2，当与我们初始化的PA和PB一样时，说明是P1和P2很有可能就是真实的值。这里面包含了两个交互的最大似然估计。

如果新估计出来的PA和PB和我们初始化的值差别很大，怎么办呢？就是继续用新的P1和P2迭代，直至收敛。

我们不妨这样，先随便给PA和PB赋一个值，比如：
```bash
硬币A正面朝上的概率PA = 0.2
硬币B正面朝上的概率PB = 0.7
```
然后，我们看看第一轮抛掷最可能是哪个硬币。
```bash
如果是硬币A，得出3正2反的概率为 0.2*0.2*0.2*0.8*0.8 = 0.00512
如果是硬币B，得出3正2反的概率为0.7*0.7*0.7*0.3*0.3=0.03087
```
然后依次求出其他4轮中的相应概率。做成表格如下（标粗表示其概率更大）：

轮数	若是硬币A	若是硬币B
```bash
1	0.00512，即0.2 0.2 0.2 0.8 0.8，3正-2反	0.03087，3正-2反
2	0.02048，即0.2 0.2 0.8 0.8 0.8，2正-3反	0.01323，2正-3反
3	0.08192，即0.2 0.8 0.8 0.8 0.8，1正-4反	0.00567，1正-4反
4	0.00512，即0.2 0.2 0.2 0.8 0.8，3正-2反	0.03087，3正-2反
5	0.02048，即0.2 0.2 0.8 0.8 0.8，2正-3反	0.01323，2正-3反
```
按照最大似然法则：
```bash
第1轮中最有可能的是硬币B
第2轮中最有可能的是硬币A
第3轮中最有可能的是硬币A
第4轮中最有可能的是硬币B
第5轮中最有可能的是硬币A
```
我们就把概率更大，即更可能是A的，即第2轮、第3轮、第5轮出现正的次数2、1、2相加，除以A被抛的总次数15（A抛了三轮，每轮5次），作为z的估计值，B的计算方法类似。然后我们便可以按照最大似然概率法则来估计新的PA和PB。
```bash
PA = （2+1+2）/15 = 0.33
PB =（3+3）/10 = 0.6
```
设想我们是全知的神，知道每轮抛掷时的硬币就是如本文本节开头标示的那样，那么，PA和PB的最大似然估计就是0.4和0.5（下文中将这两个值称为PA和PB的真实值）。那么对比下我们初始化的PA和PB和新估计出的PA和PB：

![](/images/EM_algorithm/em7.png)

看到没？我们估计的PA和PB相比于它们的初始值，更接近它们的真实值了！就这样，不断迭代 不断接近真实值，这就是EM算法的奇妙之处。

可以期待，我们继续按照上面的思路，用估计出的PA和PB再来估计z，再用z来估计新的PA和PB，反复迭代下去，就可以最终得到PA = 0.4，PB=0.5，此时无论怎样迭代，PA和PB的值都会保持0.4和0.5不变，于是乎，我们就找到了PA和PB的最大似然估计。



## 三、EM算法的公式推导

>留着下次理解，2020.4.18-0:23-四川成都